## ==============================
## 		Denodo-Gen-AI-SDK
##	    configuration parameters
## 
##   Within this file you will need to set all the necessary parameters for configuring the AI SDK.
##   Carefully go through all the sections to ensure the AI SDK works properly with your environment.
##  
##   1- API CONFIGURATION
##   2- DATA CATALOG CONFIGURATION
##   3- VECTOR STORE CONFIGURATION
##   4- LLM/EMBEDDINGS CONFIGURATION
##   5- LLM Provider configuration
## ==============================

AI_SDK_VER = 0.7

## ==============================
## 1.
## API CONFIGURATION
## Establish the host and port the API should listen from.
## ==============================

AI_SDK_HOST = 0.0.0.0
AI_SDK_PORT = 8008

## If you want to activate HTTPS, write down the key and certificate path here.

#AI_SDK_SSL_CERT = 
#AI_SDK_SSL_KEY = 

#By default, the AI SDK runs a single-threaded uvicorn server with 1 worker (thread)
#You can increase the number of workers with the AI_SDK_WORKERS parameter.
#AI_SDK_WORKERS = 4

## ==============================
## 2.
## DATA CATALOG CONFIGURATION
## Parameters that determine the connection to the Data Catalog to retrieve the metadata and execute queries.
## VDB_NAMES is a list of VDBs, separated by commas, that you want to expose to the AI SDK.
## VDB_TAGS is a list of tag names, separated by commas, that you want to expose to the AI SDK.
## Example DATA_CATALOG_URL: http://localhost:9090/denodo-data-catalog/
## ==============================

VDB_NAMES = samples_bank
#VDB_TAGS = 

DATA_CATALOG_URL = http://localhost:9090/denodo-data-catalog/

## DATA_CATALOG_SERVER_ID defines the ID of the VDP connected to the Data Catalog, it defaults to 1 if not set. 
## Only modify this property if you have multiple VDP's connected to your Data Catalog

DATA_CATALOG_SERVER_ID = 1

# If using SSL, you can set to 0 to not verify or you can include the path of the certificates to verify.

DATA_CATALOG_VERIFY_SSL = 0

##==============================
## 3.
## VECTOR STORE CONFIGURATION
## Select the vector store provider and then the specific configuration for that vector store
## Available: Chroma, PGVector, OpenSearch
##==============================

## VECTOR_STORE is the provider of the vector store

VECTOR_STORE = chroma

## Use RATE_LIMIT_RPM to set the maximum number of views to be added (and embedded) every minute

#RATE_LIMIT_RPM = 

## Use TIKTOKEN_CACHE_DIR to use the token counter model in cache 

TIKTOKEN_CACHE_DIR = "./cache/tiktoken/"

##==============================
## PGVector
##==============================

# The full connection string to the PGVector with username and password. For example: postgresql+psycopg://langchain:langchain@localhost:6024/langchain
# Must include postgresql+psycopg at the beginning. After that it's user:pwd@host:port/db

PGVECTOR_CONNECTION_STRING = 

##==============================
## OpenSearch
##==============================

# The URL of the OpenSearch instance. Default: http://localhost:9200
OPENSEARCH_URL =
OPENSEARCH_USERNAME =
OPENSEARCH_PASSWORD = 

## ==============================
## 4.
## LLM/EMBEDDINGS CONFIGURATION
## Fill in the corresponding parameters below.
## Compatible providers:
## OpenAI
## AzureOpenAI
## Bedrock
## Google
## GoogleAIStudio
## Anthropic
## NVIDIA
## Groq
## Ollama
## Mistral
## SambaNova
## OpenRouter
## + Any OpenAI compatible endpoint
## Check README for recommended models for each provider.
## ==============================

## CHAT_PROVIDER defines which LLM you will be using for simple conversation tasks.

CHAT_PROVIDER = OpenAI

## CHAT_MODEL defines the specific model ID that will be using for chat. 
##  Examples: anthropic.claude-3-5-sonnet-20240620-v1:0, gemini-1.5-flash, meta.llama2-13b-chat-v1, etc..

CHAT_MODEL = gpt-4o

## SQL_GENERATION_PROVIDER defines the specific LLM provider you will be using for the SQL generation. 

SQL_GENERATION_PROVIDER = OpenAI

## SQL_GENERATION_MODEL defines the specific model ID that will be using for the SQL generation.
## Examples: anthropic.claude-3-5-sonnet-20240620-v1:0, gemini-1.5-flash, meta.llama2-13b-chat-v1, etc..

SQL_GENERATION_MODEL = gpt-4o

## EMBEDDINGS_PROVIDER defines the specific provider you will be using for the embeddings.

EMBEDDINGS_PROVIDER = OpenAI

## EMBEDDINGS_MODEL defines the specific model ID that will be using for the embeddings.
## Example: text-embedding-3-large

EMBEDDINGS_MODEL = text-embedding-3-large

## EMBEDDINGS_TOKEN_LIMIT refers to the maximum input tokens your embedding model can receive.
## Activate this option to activate chunking of views with big schemas.
#EMBEDDINGS_TOKEN_LIMIT = 

## ==============================
## 5.
## LLM PROVIDER CONFIGURATION
## Fill in the corresponding parameters for your specific LLM provider.
## ==============================

##==============================
## Ollama
## There is no specific configuration for Ollama, but:
##      - You must have the model already installed first through Ollama
##      - You must use the same model ID in CHAT_MODEL and SQL_GENERATION_MODEL as the one you use in Ollama
## There's no need to execute 'ollama run <model-id>' to use it in the AI SDK.
## The SDK will automatically attempt to connect to the default Ollama base URL and port, but
## you can modify this using the parameter OLLAMA_API_BASE_URL.
##==============================

#OLLAMA_API_BASE_URL = http://localhost:11434

##==============================
## OpenAI
## If you want to have two different OpenAI-API compatible providers, please check the user manual.
##==============================

## OPENAI_API_KEY defines the API key for your OpenAI account.

OPENAI_API_KEY = 

## OpenAI base url can be set OPTIONALLY if using a OpenAI-compatible provider different from OpenAI.

#OPENAI_BASE_URL = 

## OpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#OPENAI_PROXY_URL = 

## OpenAI organization ID. If not set it will use the default.
#OPENAI_ORG_ID = 

## You can set the dimensions (size of the vector object) with this variable for the embeddings model.
## It is recommended to leave it to the model's default dimension size.

#OPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## AzureOpenAI
## Please set the deployment name in the CHAT_MODEL/SQL_GENERATION_MODEL variables.
## The model (deployment) name used will be appended to the Azure OpenAI endpoint, like this:
## /deployments/{CHAT_MODEL}
##==============================

## AZURE_OPENAI_ENDPOINT and AZURE_API_VERSION define the connection string and version to your AzureOpenAI instance.
## AZURE_OPENAI_ENDPOINT refers to the everything until the azure.com domain. For example: https://example-resource.openai.azure.com/
## The AI SDK will automatically append /openai/deployments/{model_name}/chat/completions... to the endpoint.

AZURE_OPENAI_ENDPOINT = 
AZURE_API_VERSION = 

##AZURE_OPENAI_API_KEY defines the API key for your OpenAI account.

AZURE_OPENAI_API_KEY = 

## AzureOpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#AZURE_OPENAI_PROXY = 

## You can set the dimensions (size of the vector object) with this variable for the embeddings model.
## It is recommended to leave it to the model's default dimension size.

#AZUREOPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## Google
## NOTE: This is Google Cloud's Vertex AI service. Meant for production.
## A JSON service account with permissions is needed as application credentials.
##==============================

## GOOGLE_APPLICATION_CREDENTIALS defines the path to the JSON storing your Google Application Credentials

GOOGLE_APPLICATION_CREDENTIALS = 

##==============================
## Google AI Studio
## NOTE: Not intended for production.
##==============================

## GOOGLE_AI_STUDIO_API_KEY is your Google AI Studio API key

GOOGLE_AI_STUDIO_API_KEY = 

##==============================
## Groq
##==============================

## GROQ_API_KEY defines the API key for your Groq account.

GROQ_API_KEY = 

##==============================
## OpenRouter
##==============================

## OPENROUTER_API_KEY defines the API key for your OpenRouter account.

OPENROUTER_API_KEY = 

## In a comma-separated list, you can specify the providers you prefer OpenRouter route your LLM calls to.
#OPENROUTER_PREFERRED_PROVIDERS = 

##==============================
## SambaNova
##==============================

## SAMBANOVA_API_KEY defines the API key for your SambaNova account.

SAMBANOVA_API_KEY = 

##==============================
## Bedrock
##==============================

## AWS_REGION, AWS_PROFILE_NAME, AWS_ROLE_ARN, AWS_SECRET_ACCESS_KEY, and AWS_ACCESS_KEY_ID define the connection parameters to your AWS Bedrock instance.
## If using EC2 with an IAM profile, you only need to set AWS_REGION to select the region you want to deploy Bedrock in.
## This is relevant because different regions have different models/costs/latencies and it is a required parameter by AWS when making a call.

AWS_REGION = 
AWS_PROFILE_NAME = 
AWS_ROLE_ARN = 
AWS_ACCESS_KEY_ID = 
AWS_SECRET_ACCESS_KEY = 

##==============================
## Mistral
##==============================

MISTRAL_API_KEY = 

##==============================
## NVIDIA NIM
##==============================

NVIDIA_API_KEY = 

# If self-hosting NVIDIA NIM, set the base url here, like "http://localhost:8000/v1"
#NVIDIA_BASE_URL = 

## ==============================
## 6.
## Extra parameters
## ==============================

# You can use CUSTOM_INSTRUCTIONS to supply the LLM with additional data it may need to correctly resolve your queries.
# This should be used as an complement to the metadata stored in Denodo, not as a substitute.
#CUSTOM_INSTRUCTIONS = 